Full name:  ____Lu__Lu____
SEAS login: ____lulu8_____

Which features did you implement? 
  (list features, or write 'Entire assignment')
Entire assignment

Did you complete any extra-credit tasks? If so, which ones?
  (list extra-credit tasks)

DFA-based XPath engine is implemented using a SAX parser.

Provides a Web interface for the crawler.(Please see details below)

Any special instructions for building and running your solution?
  (include detailed instructions, or write 'None')

The servlet runs on Tomcat 7.0. The url-servlet mapping information can be found in WebContent/WEB-INF/web.xml. 

To run the crawler, simply run the XPathCrawler.java. It needs 3 parameters: the start url, the directory of the DB store and the maxmium file size. The DB store directory must be complete and looks something like /home/... It must be the same with the DB directory specified in WebContent/WEB-INF/web.xml. By default the crawler will stop when there is no url anymore or it has crawled 1000 web pages.

To run the servlets, right click the HW2 project-> Run as-> Run on server. Then go to 
http://localhost:8080/HW2/login 
to login. First you need to register an account through the register button.
After login, you can create a channel and specify its parameters.

The Xpaths are separated by semicolon, something like:

/rss[@version="2.0"]/channel/item/title[contains(text(), "war")]; /rss[@version="2.0"]/channel/item/title[contains(text(), "peace")]; /rss[@version="2.0"]/channel/item/description[contains(text(), "war")]; /rss[@version="2.0"]/channel/item/description[contains(text(), "peace")]

The XSLT url should be a relative path whose root is WebContent folder. For example, to access /WebContent/rss/rss.xsl, simply input:
rss/rss.xsl

After creating a channel, you can start the crawler and it will feed the channel with content. Otherwise the channel is empty.

The rss folder is under WebContent folder, inside it there's warandpeace.xp and rss.xsl files

To access to the crawler interface(extra credit part), type in admin as the username and admin as the password. After login you can see the information of the crawled data and start or stop the crawler. And you can also specify the crawler parameters.

Did you personally write _all_ the code you are submitting
(other than code from the course web page)?
  [X] Yes
  [ ] No

Did you copy any code from the Internet, or from classmates?
  [ ] Yes
  [X] No

Did you collaborate with anyone on this assignment?
  [ ] Yes
  [X] No
